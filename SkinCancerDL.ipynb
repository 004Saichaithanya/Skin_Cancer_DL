{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 332046,
          "sourceType": "datasetVersion",
          "datasetId": 141236
        },
        {
          "sourceId": 244755,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 209095,
          "modelId": 230788
        }
      ],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "SkinCancerDL",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/004Saichaithanya/Skin_Cancer_DL/blob/main/SkinCancerDL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "artakusuma_basedir_path = kagglehub.dataset_download('artakusuma/basedir')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "pu-zmPCgRsFm"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "3PclDZObRsFo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"artakusuma/basedir\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UEdutwV6RsFo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "kl7xUoeiRsFp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "SGP1BLoMRsFp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "physical_devices"
      ],
      "metadata": {
        "trusted": true,
        "id": "JOzmuIxLRsFq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.applications import ResNet50\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# from tensorflow.keras.models import load_model\n",
        "# import tensorflow as tf\n",
        "\n",
        "# # Parameters\n",
        "# img_size = (224, 224)\n",
        "# batch_size = 32\n",
        "# num_classes = 7\n",
        "# epochs = 7\n",
        "# # Data generators with extended augmentation\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#     rescale=1.0 / 255.0,\n",
        "#     rotation_range=40,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     shear_range=0.2,\n",
        "#     zoom_range=0.3,\n",
        "#     horizontal_flip=True,\n",
        "#     fill_mode='nearest',\n",
        "#     brightness_range=[0.5, 1.5],\n",
        "#     channel_shift_range=50.0,\n",
        "# )\n",
        "\n",
        "# valid_datagen = ImageDataGenerator(rescale=1.0 / 255.0)i\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "QrwJ5RLtRsFq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Data loaders\n",
        "# train_generator = train_datagen.flow_from_directory(\n",
        "#     train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', shuffle=True\n",
        "# )\n",
        "# valid_generator = valid_datagen.flow_from_directory(\n",
        "#     val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', shuffle=False\n",
        "# )\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "JV6uDOUERsFr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load ResNet50 model\n",
        "# base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "# # Add classification layers\n",
        "# x = GlobalAveragePooling2D()(base_model.output)\n",
        "# x = Dropout(0.3)(x)\n",
        "# output = Dense(num_classes, activation='softmax')(x)\n",
        "# model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# # Focal Loss function\n",
        "# @tf.keras.utils.register_keras_serializable()\n",
        "# def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
        "#     y_true = tf.cast(y_true, tf.float32)\n",
        "#     y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "#     cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "#     weight = alpha * tf.pow(1 - y_pred, gamma)\n",
        "#     return tf.reduce_mean(weight * cross_entropy, axis=-1)\n",
        "\n",
        "# # Compile the model with Adam optimizer\n",
        "# model.compile(optimizer=Adam(learning_rate=0.001), loss=focal_loss, metrics=['accuracy'])\n",
        "# # Callbacks for early stopping and model checkpoint\n",
        "# callbacks = [\n",
        "#     EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "#     ModelCheckpoint('best_model_resnet.keras', monitor='val_loss', save_best_only=True)\n",
        "# ]\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(\n",
        "#     train_generator,\n",
        "#     validation_data=valid_generator,\n",
        "#     epochs=epochs,\n",
        "#     steps_per_epoch=train_generator.samples // batch_size,\n",
        "#     validation_steps=valid_generator.samples // batch_size,\n",
        "#     callbacks=callbacks\n",
        "# )\n",
        "\n",
        "# # Load the best model saved during training\n",
        "# best_model = load_model('best_model_resnet.keras', custom_objects={'focal_loss': focal_loss})\n",
        "\n",
        "# # Evaluate on the validation set using the best model\n",
        "# loss, accuracy = best_model.evaluate(valid_generator)\n",
        "# print(f\"Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "y3QUOJKxRsFr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Accuracy Plot\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(epochs_range, history.history['accuracy'], label=\"Training Accuracy\")\n",
        "# plt.plot(epochs_range, history.history['val_accuracy'], label=\"Validation Accuracy\")\n",
        "# plt.title(\"Model Accuracy\")\n",
        "# plt.xlabel(\"Epochs\")\n",
        "# plt.ylabel(\"Accuracy\")\n",
        "# plt.legend()\n",
        "\n",
        "# # Loss Plot\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(epochs_range, history.history['loss'], label=\"Training Loss\")\n",
        "# plt.plot(epochs_range, history.history['val_loss'], label=\"Validation Loss\")\n",
        "# plt.title(\"Model Loss\")\n",
        "# plt.xlabel(\"Epochs\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "l4RMNdtHRsFs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count images in each class\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def count_images_in_classes(folder):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(folder):\n",
        "        class_path = os.path.join(folder, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "# Count images\n",
        "train_counts = count_images_in_classes(train_folder)\n",
        "val_counts = count_images_in_classes(val_folder)\n",
        "\n",
        "# Create Data Distribution Plot\n",
        "def plot_data_distribution(counts, title):\n",
        "    classes = list(counts.keys())\n",
        "    values = list(counts.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(classes, values, color='skyblue')\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for Training Data\n",
        "plot_data_distribution(train_counts, \"Training Data Distribution\")\n",
        "\n",
        "# Plot for Validation Data\n",
        "plot_data_distribution(val_counts, \"Validation Data Distribution\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "3-4nvXpVRsFt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from collections import Counter\n",
        "from torchvision.datasets import ImageFolder\n",
        "from collections import defaultdict\n",
        "# # Define directories\n",
        "# output_dir = '/kaggle/working/balanced_dir'\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# train_dataset = ImageFolder(train_dir)\n",
        "# train_classes = set(train_dataset.classes)\n",
        "\n",
        "# val_dataset = ImageFolder(val_dir)\n",
        "# val_class_counts = Counter([label for _, label in val_dataset])\n",
        "\n",
        "# for class_name in train_classes:\n",
        "#     if class_name not in val_dataset.class_to_idx:\n",
        "#         val_class_counts[len(val_class_counts)] = 0  # Assign a new label\n",
        "\n",
        "# max_class_count = max(val_class_counts.values())\n",
        "# label_to_class = {v: k for k, v in val_dataset.class_to_idx.items()}\n",
        "\n",
        "# for class_name in train_classes:\n",
        "#     val_class_dir = os.path.join(output_dir, 'val_dir', class_name)\n",
        "#     os.makedirs(val_class_dir, exist_ok=True)\n",
        "\n",
        "# for label, count in val_class_counts.items():\n",
        "#     class_name = label_to_class[label]\n",
        "#     train_class_dir = os.path.join(train_dir, class_name)\n",
        "#     val_class_dir = os.path.join(output_dir, 'val_dir', class_name)\n",
        "\n",
        "#     if count == 0:\n",
        "#         existing_val_images = []\n",
        "#     else:\n",
        "#         existing_val_images = os.listdir(val_class_dir)\n",
        "\n",
        "#     num_to_add = max_class_count - len(existing_val_images)\n",
        "\n",
        "#     if num_to_add > 0:\n",
        "#         train_images = os.listdir(train_class_dir)\n",
        "#         sampled_images = random.choices(train_images, k=num_to_add)\n",
        "\n",
        "#         for img in sampled_images:\n",
        "#             src_path = os.path.join(train_class_dir, img)\n",
        "#             dst_path = os.path.join(val_class_dir, f\"aug_{img}\")\n",
        "#             shutil.copy(src_path, dst_path)\n",
        "\n",
        "# print(\"Validation dataset has been balanced with all training classes.\")\n",
        "\n",
        "\n",
        "output_dir = \"/kaggle/working/total_dir\"   # New directory to hold all images\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "lno4HHw-RsFt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total_folder = \"/kaggle/working/total_dir\"\n",
        "def count_images(folder):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(folder):\n",
        "        class_path = os.path.join(folder,class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "total_val_counts = count_images_in_classes(total_folder)\n",
        "def plot_data_distribution(counts, title):\n",
        "    classes = list(counts.keys())\n",
        "    values = list(counts.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(classes, values, color='skyblue')\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_data_distribution(total_val_counts, \"Total Data Distributions\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "KxHyN_Z6RsFt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "class_order = ['akiec', 'bcc', 'nv', 'df', 'mel', 'vasc', 'bkl']\n",
        "\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image, label\n",
        "\n",
        "total_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    total_folder,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',\n",
        "    shuffle=False,\n",
        "    class_names=class_order\n",
        ").map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ghLMoOASRsFu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "5I9bdygdRsFu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "0RM0lYf9RsFu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "vX227lqjRsFu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "A3cfAcoGRsFu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "oAF7uWFHRsFv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "XN89EsnIRsFv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "val_folder = val_dir = \"/kaggle/working/balanced_val_dir/val_dir/\"\n",
        "# Function to count images in each class\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def count_images_in_classes(folder):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(folder):\n",
        "        class_path = os.path.join(folder, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "# Count images\n",
        "train_counts = count_images_in_classes(train_folder)\n",
        "val_counts = count_images_in_classes(val_folder)\n",
        "\n",
        "# Create Data Distribution Plot\n",
        "def plot_data_distribution(counts, title):\n",
        "    classes = list(counts.keys())\n",
        "    values = list(counts.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(classes, values, color='skyblue')\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for Training Data\n",
        "plot_data_distribution(train_counts, \"Training Data Distribution\")\n",
        "\n",
        "# Plot for Validation Data\n",
        "plot_data_distribution(val_counts, \"Validation Data Distribution\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "n09kJARcRsFv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, BatchNormalization, LeakyReLU, Dropout\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.applications import EfficientNetB0\n",
        "# from tensorflow.keras.optimizers import Adam, SGD\n",
        "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# import tensorflow as tf\n",
        "# import os\n",
        "# # Parameters\n",
        "# IMG_SIZE = (128, 128)\n",
        "# BATCH_SIZE = 32\n",
        "# class_order = ['akiec', 'bcc', 'nv', 'df', 'mel', 'vasc', 'bkl']\n",
        "# NUM_CLASSES = len(class_order)\n",
        "\n",
        "# # Define the class order explicitly\n",
        "#   # Replace with your actual class names\n",
        "\n",
        "# # Data augmentation\n",
        "# train_datagen = ImageDataGenerator(\n",
        "#     rescale=1.0/255,\n",
        "#     horizontal_flip=True,\n",
        "#     zoom_range=0.2,\n",
        "#     rotation_range=10,\n",
        "#     width_shift_range=0.2,\n",
        "#     height_shift_range=0.2,\n",
        "#     shear_range=0.2,\n",
        "#     brightness_range=[0.8, 1.2],\n",
        "#     channel_shift_range=20.0,\n",
        "# )\n",
        "\n",
        "# val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# train_generator = train_datagen.flow_from_directory(\n",
        "#     train_folder,\n",
        "#     target_size=IMG_SIZE,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     class_mode='categorical',\n",
        "#     classes = class_order\n",
        "# )\n",
        "\n",
        "# val_generator = val_datagen.flow_from_directory(\n",
        "#     val_folder,\n",
        "#     target_size=IMG_SIZE,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     class_mode='categorical',\n",
        "#     classes = class_order\n",
        "# )\n",
        "\n",
        "# print(\"Training class indices:\", train_generator.class_indices)\n",
        "# print(\"Validation class indices:\", val_generator.class_indices)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NKIhvsCqRsFv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load DenseNet121 base model (you can use DenseNet169, DenseNet201, etc.)\n",
        "# from tensorflow.keras.applications import DenseNet121\n",
        "\n",
        "# base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "# base_model.trainable = False  # Freeze all layers initially\n",
        "\n",
        "# # Custom layers\n",
        "# x = base_model.output\n",
        "# x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# # Dense Layer 1\n",
        "# x = Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = LeakyReLU(alpha=0.1)(x)\n",
        "# x = Dropout(0.4)(x)\n",
        "\n",
        "# # Dense Layer 2\n",
        "# x = Dense(64, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = LeakyReLU(alpha=0.1)(x)\n",
        "# x = Dropout(0.4)(x)\n",
        "\n",
        "# # Output Layer\n",
        "# predictions = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "# # Final Model\n",
        "# model = Model(inputs=base_model.input, outputs=predictions)\n",
        "# model.summary()\n",
        "# model = tf.keras.models.load_model(\"/kaggle/input/densenet_pret_wus/tensorflow1/default/1/my_model.h5\")\n",
        "# new_output = model.layers[-4].output  # Use the output of the 4th-to-last layer\n",
        "# base_model = tf.keras.Model(inputs=model.input, outputs=new_output)\n",
        "\n",
        "# # Step 3: Add custom layers\n",
        "# x = base_model.output\n",
        "# x = Dense(128, activation='relu', name=\"dense_128_relu\")(x)\n",
        "# x = Dropout(0.5, name=\"dropout_50\")(x)\n",
        "# predictions = Dense(NUM_CLASSES, activation='softmax', name=\"dense_soft_max\")(x)\n",
        "# new_model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
        "# # Compile with a fixed learning rate\n",
        "# optimizer = Adam(learning_rate=1e-3)\n",
        "# new_model.compile(\n",
        "#     optimizer=optimizer,\n",
        "#     loss='categorical_crossentropy',\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# # Callbacks\n",
        "# reduce_lr = ReduceLROnPlateau(\n",
        "#     monitor='val_accuracy',\n",
        "#     factor=0.5,\n",
        "#     patience=5,\n",
        "#     verbose=1,\n",
        "#     min_lr=1e-7\n",
        "# )\n",
        "\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_accuracy',\n",
        "#     patience=9,\n",
        "#     restore_best_weights=True,\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# # Train the model initially with base layers frozen\n",
        "# history = new_model.fit(\n",
        "#     train_generator,\n",
        "#     validation_data=val_generator,\n",
        "#     epochs=20,\n",
        "#     callbacks=[reduce_lr, early_stopping],\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# # Gradual unfreezing: Start unfreezing layers one-by-one or in batches\n",
        "# for layer in base_model.layers[-30:]:  # Unfreeze the last 30 layers\n",
        "#     layer.trainable = True\n",
        "\n",
        "# # Recompile with a lower learning rate for fine-tuning\n",
        "# fine_tune_optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
        "# model.compile(\n",
        "#     optimizer=fine_tune_optimizer,\n",
        "#     loss='categorical_crossentropy',\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# # Fine-tune the model\n",
        "# early_stopping_fine = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_accuracy',\n",
        "#     patience=5,\n",
        "#     restore_best_weights=True,\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# reduce_lr_fine = ReduceLROnPlateau(\n",
        "#     monitor='val_accuracy',\n",
        "#     factor=0.5,\n",
        "#     patience=3,\n",
        "#     verbose=1,\n",
        "#     min_lr=1e-7\n",
        "# )\n",
        "\n",
        "\n",
        "# # Step 4: Create the final model\n",
        "# new_model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# # Step 5: Compile and train\n",
        "# new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history_fine = model.fit(\n",
        "#     train_generator,\n",
        "#     validation_data=val_generator,\n",
        "#     epochs=2,\n",
        "#     callbacks=[reduce_lr_fine, early_stopping_fine],\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# # Evaluate the model\n",
        "# train_loss, train_accuracy = model.evaluate(train_generator, verbose=1)\n",
        "# print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# test_loss, test_accuracy = model.evaluate(val_generator, verbose=1)\n",
        "# print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "# y_pred = model.predict(val_generator)\n",
        "# y_true = val_generator.classes\n",
        "# print(classification_report(y_true, y_pred.argmax(axis=1)))\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "# cm = confusion_matrix(y_true, y_pred.argmax(axis=1))\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_generator.class_indices.keys())\n",
        "# disp.plot(cmap='viridis')\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # After training\n",
        "# plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "# plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(history.history['loss'], label='Train Loss')\n",
        "# plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ow4AY9oYRsFv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save('/kaggle/working/my_model.h5')\n",
        "# import shutil\n",
        "# shutil.make_archive('/kaggle/working/my_model', 'zip', '/kaggle/working/my_model')"
      ],
      "metadata": {
        "trusted": true,
        "id": "JHEIBjdERsFw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from PIL import Image\n",
        "\n",
        "# def check_corrupted_images(directory):\n",
        "#     corrupted_files = []\n",
        "#     for root, _, files in os.walk(directory):\n",
        "#         for file in files:\n",
        "#             file_path = os.path.join(root, file)\n",
        "#             try:\n",
        "#                 img = Image.open(file_path)\n",
        "#                 img.verify()  # Verify the image integrity\n",
        "#                 img.close()\n",
        "#             except (IOError, OSError, Image.DecompressionBombError):\n",
        "#                 corrupted_files.append(file_path)\n",
        "#     return corrupted_files\n",
        "\n",
        "# # Check training and validation directories\n",
        "# train_corrupted = check_corrupted_images(train_folder)\n",
        "# val_corrupted = check_corrupted_images(val_folder)\n",
        "\n",
        "# print(\"Corrupted training images:\", train_corrupted)\n",
        "# print(\"Corrupted validation images:\", val_corrupted)\n",
        "\n",
        "# Remove corrupted files\n",
        "# for file in train_corrupted + val_corrupted:\n",
        "#     os.remove(file)\n",
        "#     print(f\"Removed: {file}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "0KzsqBpmRsFw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "val_folder=\"/kaggle/working//balanced_val_dir/val_dir\"\n",
        "# Define image size and batch size\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "class_order = ['akiec', 'bcc', 'nv', 'df', 'mel', 'vasc', 'bkl']\n",
        "# Function to preprocess the image tensor (no file reading needed)\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image, label\n",
        "\n",
        "# Create a dataset from directory (images are already loaded as tensors)\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_folder,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',\n",
        "    class_names=class_order\n",
        ").map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_folder,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',\n",
        "    class_names=class_order\n",
        ").map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "trusted": true,
        "id": "I0FSFIOwRsFw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "train_ds = train_ds.apply(tf.data.experimental.ignore_errors())\n",
        "val_ds = val_ds.apply(tf.data.experimental.ignore_errors())\n",
        "# Load pre-trained model\n",
        "model = tf.keras.models.load_model(\"/kaggle/input/densenet_pret_wus/tensorflow1/default/1/my_model.h5\")\n",
        "new_output = model.layers[-4].output  # Use the output of the 4th-to-last layer\n",
        "base_model = tf.keras.Model(inputs=model.input, outputs=new_output)\n",
        "\n",
        "# Step 3: Add custom layers\n",
        "x = base_model.output\n",
        "x = Dense(128, activation='relu', name=\"dense_128_relu\")(x)\n",
        "x = Dropout(0.5, name=\"dropout_50\")(x)\n",
        "predictions = Dense(7, activation='softmax', name=\"dense_soft_max\")(x)  # Replaced NUM_CLASSES with 7\n",
        "new_model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile with a fixed learning rate\n",
        "optimizer = Adam(learning_rate=1e-3)\n",
        "new_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    min_lr=1e-7\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=9,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model initially with base layers frozen\n",
        "history = new_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20,\n",
        "    callbacks=[reduce_lr, early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Gradual unfreezing: Start unfreezing layers one-by-one or in batches\n",
        "for layer in base_model.layers[-30:]:  # Unfreeze the last 30 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with a lower learning rate for fine-tuning\n",
        "fine_tune_optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
        "new_model.compile(\n",
        "    optimizer=fine_tune_optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "early_stopping_fine = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr_fine = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    min_lr=1e-7\n",
        ")\n",
        "\n",
        "# Step 4: Train the fine-tuned model\n",
        "history_fine = new_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=2,\n",
        "    callbacks=[reduce_lr_fine, early_stopping_fine],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# # Evaluate the model\n",
        "# train_loss, train_accuracy = new_model.evaluate(train_ds, verbose=1)\n",
        "# print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# test_loss, test_accuracy = new_model.evaluate(val_ds, verbose=1)\n",
        "# print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "# y_pred = new_model.predict(val_ds)  # Use the correct dataset\n",
        "# y_true = val_ds.classes  # Ensure val_ds has `.classes` attribute\n",
        "# print(classification_report(y_true, y_pred.argmax(axis=1)))\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "# cm = confusion_matrix(y_true, y_pred.argmax(axis=1))\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_ds.class_indices.keys())\n",
        "# disp.plot(cmap='viridis')\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # After training\n",
        "# plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "# plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(history.history['loss'], label='Train Loss')\n",
        "# plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "3mlIzck1RsFw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "val_orig = \"/kaggle/input/basedir/base_dir/val_dir\"\n",
        "val_original_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    val_orig,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',\n",
        "    shuffle=False,\n",
        "    class_names=class_order\n",
        ").map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "MuWVAi7kRsFx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After training\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_DQcnwXcRsFx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model initially with base layers frozen\n",
        "history = new_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=2,\n",
        "    callbacks=[reduce_lr, early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Gradual unfreezing: Start unfreezing layers one-by-one or in batches\n",
        "for layer in base_model.layers[-30:]:  # Unfreeze the last 30 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with a lower learning rate for fine-tuning\n",
        "fine_tune_optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
        "new_model.compile(\n",
        "    optimizer=fine_tune_optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "early_stopping_fine = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr_fine = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    min_lr=1e-7\n",
        ")\n",
        "\n",
        "# Step 4: Train the fine-tuned model\n",
        "history_fine = new_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=6,\n",
        "    callbacks=[reduce_lr_fine, early_stopping_fine],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "yveXp_6ZRsFx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Predict on validation set\n",
        "y_pred = new_model.predict(val_original_ds)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Extract true labels (one-hot to class indices)\n",
        "y_true = np.concatenate([y.numpy() for _, y in val_original_ds], axis=0)\n",
        "y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_true, y_pred_classes, target_names=class_order))"
      ],
      "metadata": {
        "trusted": true,
        "id": "tMx8ItL_RsFx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['akiec', 'bcc', 'nv', 'df', 'mel', 'vasc', 'bkl'])\n",
        "disp.plot(cmap='viridis')"
      ],
      "metadata": {
        "trusted": true,
        "id": "90Ehe_P7RsFx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = sum(tf.shape(x)[0].numpy() for x, _ in val_ds)\n",
        "print(f\"Total images in val_ds after ignoring errors: {num_samples}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "3csRx9GuRsFy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = sum(tf.shape(x)[0].numpy() for x, _ in train_ds)\n",
        "print(f\"Total images in train_ds after ignoring errors: {num_samples}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "n4dyCLrVRsFy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "new_model.save('/kaggle/working/final_model.h5')"
      ],
      "metadata": {
        "trusted": true,
        "id": "eVNA-tScRsFy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "n3Z9Cry2RsFy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fresh Model"
      ],
      "metadata": {
        "id": "kht7BZaJRsFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.callbacks as callbacks\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow.keras.optimizers\n",
        "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,ZeroPadding2D,Activation,Flatten,Dropout,GlobalAveragePooling2D,BatchNormalization\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications import ResNet50,MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import zipfile\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import scipy"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T08:40:17.916779Z",
          "iopub.execute_input": "2025-01-31T08:40:17.917086Z",
          "iopub.status.idle": "2025-01-31T08:40:17.936287Z",
          "shell.execute_reply.started": "2025-01-31T08:40:17.917035Z",
          "shell.execute_reply": "2025-01-31T08:40:17.935453Z"
        },
        "id": "rtl1ReRsRsF0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/kaggle/input/basedir/base_dir/train_dir\"\n",
        "val_dir = \"/kaggle/working/balanced_val_dir/val_dir\"\n",
        "total_dir = \"/kaggle/working/total_dir\"\n",
        "metadata=\"/kaggle/input/metadata/HAM10000_metadata.csv\"\n",
        "total_folder = total_dir\n",
        "train_folder = train_dir\n",
        "val_folder = val_dir"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T08:43:20.90036Z",
          "iopub.execute_input": "2025-01-31T08:43:20.900685Z",
          "iopub.status.idle": "2025-01-31T08:43:20.904603Z",
          "shell.execute_reply.started": "2025-01-31T08:43:20.900663Z",
          "shell.execute_reply": "2025-01-31T08:43:20.903623Z"
        },
        "id": "-8e9Av-_RsF0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use this if total dir is not present in Output Folder\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from collections import Counter\n",
        "from torchvision.datasets import ImageFolder\n",
        "from collections import defaultdict\n",
        "### Step 1: Collect all classes from train and val directories\n",
        "def get_classes_and_counts(directory):\n",
        "    classes = defaultdict(int)\n",
        "    if os.path.exists(directory):\n",
        "        for class_name in os.listdir(directory):\n",
        "            class_path = os.path.join(directory, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                classes[class_name] = len(os.listdir(class_path))\n",
        "    return classes\n",
        "\n",
        "train_classes = get_classes_and_counts(train_dir)\n",
        "val_classes = get_classes_and_counts(val_dir)\n",
        "all_classes = set(list(train_classes.keys()) + list(val_classes.keys()))\n",
        "\n",
        "### Step 2: Copy all images from train_dir and val_dir into output_dir\n",
        "for class_name in all_classes:\n",
        "    # Create class subdirectory in output_dir\n",
        "    output_class_dir = os.path.join(output_dir, class_name)\n",
        "    os.makedirs(output_class_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy images from train_dir (if they exist)\n",
        "    train_class_dir = os.path.join(train_dir, class_name)\n",
        "    if os.path.exists(train_class_dir):\n",
        "        for img in os.listdir(train_class_dir):\n",
        "            src = os.path.join(train_class_dir, img)\n",
        "            dst = os.path.join(output_class_dir, f\"train_{img}\")  # Avoid name conflicts\n",
        "            shutil.copy(src, dst)\n",
        "    \n",
        "    # Copy images from val_dir (if they exist)\n",
        "    val_class_dir = os.path.join(val_dir, class_name)\n",
        "    if os.path.exists(val_class_dir):\n",
        "        for img in os.listdir(val_class_dir):\n",
        "            src = os.path.join(val_class_dir, img)\n",
        "            dst = os.path.join(output_class_dir, f\"val_{img}\")  # Avoid name conflicts\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "print(f\"Combined dataset created at: {output_dir}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-30T13:29:23.189025Z",
          "iopub.execute_input": "2025-01-30T13:29:23.189339Z",
          "iopub.status.idle": "2025-01-30T13:29:23.23192Z",
          "shell.execute_reply.started": "2025-01-30T13:29:23.18931Z",
          "shell.execute_reply": "2025-01-30T13:29:23.230831Z"
        },
        "id": "nxOxZsGSRsF1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "aqhgDvXERsF1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count images in each class\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def count_images_in_classes(folder):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(folder):\n",
        "        class_path = os.path.join(folder, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "# Count images\n",
        "train_counts = count_images_in_classes(train_dir)\n",
        "val_counts = count_images_in_classes(val_dir)\n",
        "total_counts = count_images_in_classes(total_dir)\n",
        "\n",
        "# Create Data Distribution Plot\n",
        "def plot_data_distribution(counts, title):\n",
        "    classes = list(counts.keys())\n",
        "    values = list(counts.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(classes, values, color='skyblue')\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for Training Data\n",
        "plot_data_distribution(train_counts, \"Training Data Distribution\")\n",
        "\n",
        "# Plot for Validation Data\n",
        "plot_data_distribution(val_counts, \"Validation Data Distribution\")\n",
        "\n",
        "# Plot for Total Data\n",
        "plot_data_distribution(total_counts, \"Total Data Distribution\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T08:43:22.550549Z",
          "iopub.execute_input": "2025-01-31T08:43:22.550831Z",
          "iopub.status.idle": "2025-01-31T08:43:23.474518Z",
          "shell.execute_reply.started": "2025-01-31T08:43:22.550809Z",
          "shell.execute_reply": "2025-01-31T08:43:23.47381Z"
        },
        "id": "vKvB62PKRsF1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Define constants\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "KAGGLE_OUTPUT_DIR = \"/kaggle/working/saved_data\"\n",
        "\n",
        "# Create directories to save images and metadata\n",
        "os.makedirs(KAGGLE_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load the full dataset without batching\n",
        "full_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    total_dir,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=None,  # Load individual samples\n",
        "    label_mode='categorical',\n",
        "    shuffle=False  # Do not shuffle here; we'll shuffle later during splitting\n",
        ")\n",
        "\n",
        "# Extract class names\n",
        "class_names = full_ds.class_names\n",
        "\n",
        "# Lists to store file paths and labels\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "# Save images and labels\n",
        "for i, (img, label) in enumerate(full_ds):\n",
        "    # Save the image to disk\n",
        "    img_path = os.path.join(KAGGLE_OUTPUT_DIR, f\"image_{i}.jpg\")\n",
        "    tf.keras.utils.save_img(img_path, img.numpy())\n",
        "\n",
        "    # Store the file path and label\n",
        "    file_paths.append(img_path)\n",
        "    labels.append(label.numpy())\n",
        "\n",
        "# Convert labels to NumPy array\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T08:54:25.948043Z",
          "iopub.execute_input": "2025-01-31T08:54:25.948444Z",
          "iopub.status.idle": "2025-01-31T08:55:39.897245Z",
          "shell.execute_reply.started": "2025-01-31T08:54:25.948414Z",
          "shell.execute_reply": "2025-01-31T08:55:39.896322Z"
        },
        "id": "lO98UKjiRsF1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform train-test split (e.g., 80% train, 20% validation)\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    file_paths, labels,\n",
        "    test_size=0.2,  # 20% for validation\n",
        "    random_state=SEED,\n",
        "    stratify=np.argmax(labels, axis=1)  # Stratify by class indices\n",
        ")\n",
        "\n",
        "def load_image_and_label(image_path, label):\n",
        "    # Load and preprocess the image\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image, label\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "train_ds = train_ds.map(load_image_and_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "val_ds = val_ds.map(load_image_and_label, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Apply batching and prefetching\n",
        "train_ds = train_ds.batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(64).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T08:59:05.327971Z",
          "iopub.execute_input": "2025-01-31T08:59:05.328295Z",
          "iopub.status.idle": "2025-01-31T08:59:05.447034Z",
          "shell.execute_reply.started": "2025-01-31T08:59:05.328272Z",
          "shell.execute_reply": "2025-01-31T08:59:05.446408Z"
        },
        "id": "gBjUKYesRsF2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training dataset size:\", len(train_paths))\n",
        "print(\"Validation dataset size:\", len(val_paths))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T08:59:08.17812Z",
          "iopub.execute_input": "2025-01-31T08:59:08.178436Z",
          "iopub.status.idle": "2025-01-31T08:59:08.183382Z",
          "shell.execute_reply.started": "2025-01-31T08:59:08.178414Z",
          "shell.execute_reply": "2025-01-31T08:59:08.182354Z"
        },
        "id": "VztGZRdCRsF2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "def extract_labels(dataset):\n",
        "    labels = []\n",
        "    for _, label_batch in dataset.unbatch():\n",
        "        labels.append(np.argmax(label_batch.numpy()))\n",
        "    return np.array(labels)\n",
        "\n",
        "train_labels = extract_labels(train_ds)\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight = 'balanced',\n",
        "    classes = np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "print('Class Weights:', class_weight_dict)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T09:01:10.098606Z",
          "iopub.execute_input": "2025-01-31T09:01:10.098943Z",
          "iopub.status.idle": "2025-01-31T09:01:22.099917Z",
          "shell.execute_reply.started": "2025-01-31T09:01:10.098919Z",
          "shell.execute_reply": "2025-01-31T09:01:22.099166Z"
        },
        "id": "awvzm3m_RsF2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "wP2Dv7vPRsF2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "w1L3P2DWRsF2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# #Data Structure Creation\n",
        "# IMG_SIZE = (224, 224)\n",
        "# BATCH_SIZE = 32\n",
        "# class_order = ['akiec', 'bcc', 'nv', 'df', 'mel', 'vasc', 'bkl']\n",
        "\n",
        "# def preprocess_image(image, label):\n",
        "#     image = tf.image.resize(image, IMG_SIZE)\n",
        "#     image = image / 255.0  # Normalize to [0, 1]\n",
        "#     return image, label\n",
        "\n",
        "# # Create a dataset from directory (images are already loaded as tensors)\n",
        "# train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "#     train_folder,\n",
        "#     image_size=IMG_SIZE,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     label_mode='categorical',\n",
        "#     shuffle=True,\n",
        "#     class_names=class_order\n",
        "# ).map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "#     val_folder,\n",
        "#     image_size=IMG_SIZE,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle=True,\n",
        "#     label_mode='categorical',\n",
        "#     class_names=class_order\n",
        "# ).map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "# val_orig_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "#     \"/kaggle/input/basedir/base_dir/val_dir\",\n",
        "#     image_size = IMG_SIZE,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle = True,\n",
        "#     label_mode='categorical',\n",
        "#     class_names=class_order\n",
        "# ).map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# # total_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "# #     total_folder,\n",
        "# #     image_size=IMG_SIZE,\n",
        "# #     batch_size=BATCH_SIZE,\n",
        "# #     label_mode='categorical',\n",
        "# #     shuffle=False,\n",
        "# #     class_names=class_order\n",
        "# # ).map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T08:43:33.765129Z",
          "iopub.execute_input": "2025-01-31T08:43:33.765419Z",
          "iopub.status.idle": "2025-01-31T08:43:59.032756Z",
          "shell.execute_reply.started": "2025-01-31T08:43:33.765397Z",
          "shell.execute_reply": "2025-01-31T08:43:59.032067Z"
        },
        "_kg_hide-input": true,
        "id": "xu8aYEMXRsF2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, BatchNormalization, LeakyReLU, Dropout\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.applications import EfficientNetB0\n",
        "# from tensorflow.keras.optimizers import Adam, SGD\n",
        "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# import tensorflow as tf\n",
        "# # Load DenseNet121 base model (you can use DenseNet169, DenseNet201, etc.)\n",
        "# from tensorflow.keras.applications import DenseNet121\n",
        "\n",
        "# model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "# model.trainable = False  # Freeze all layers initially\n",
        "# train_ds = train_ds.apply(tf.data.experimental.ignore_errors())\n",
        "# val_ds = val_ds.apply(tf.data.experimental.ignore_errors())\n",
        "# x = model.output\n",
        "# x = GlobalAveragePooling2D()(x)\n",
        "# x = Dense(128, activation='relu', name=\"dense_128_relu\")(x)\n",
        "# x = Dropout(0.5, name=\"dropout_50\")(x)\n",
        "# predictions = Dense(7, activation='softmax', name=\"dense_soft_max\")(x)  # Replaced NUM_CLASSES with 7\n",
        "# model = tf.keras.Model(inputs=model.input, outputs=predictions)\n",
        "optimizer = Adam(learning_rate=1e-3)\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    verbose=1,\n",
        "    min_lr=1e-7\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=9,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train the model initially with base layers frozen\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15,\n",
        "    callbacks=[reduce_lr, early_stopping],\n",
        "    class_weight = class_weight_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Gradual unfreezing: Start unfreezing layers one-by-one or in batches\n",
        "for layer in model.layers[-30:]:  # Unfreeze the last 30 layers\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile with a lower learning rate for fine-tuning\n",
        "fine_tune_optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=fine_tune_optimizer,\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "early_stopping_fine = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr_fine = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    min_lr=1e-7\n",
        ")\n",
        "\n",
        "\n",
        "# Step 5: Compile and train\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history_fine = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10,\n",
        "    class_weight = class_weight_dict,\n",
        "    callbacks=[reduce_lr_fine, early_stopping_fine],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T09:17:16.72341Z",
          "iopub.execute_input": "2025-01-31T09:17:16.723733Z",
          "iopub.status.idle": "2025-01-31T09:40:34.245383Z",
          "shell.execute_reply.started": "2025-01-31T09:17:16.723705Z",
          "shell.execute_reply": "2025-01-31T09:40:34.244579Z"
        },
        "id": "5dCn4r2qRsF2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure the dataset is unbatched for proper iteration\n",
        "val_ds_unbatched = val_ds.unbatch()\n",
        "\n",
        "# Extract true labels\n",
        "y_true = np.array([label.numpy() for _, label in val_ds_unbatched])\n",
        "y_true = np.argmax(y_true, axis=1)  # Convert one-hot to class indices\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = model.predict(val_ds)\n",
        "y_pred = np.argmax(y_pred, axis=1)  # Convert probabilities to class indices"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T09:40:34.246735Z",
          "iopub.execute_input": "2025-01-31T09:40:34.247021Z",
          "iopub.status.idle": "2025-01-31T09:40:59.559586Z",
          "shell.execute_reply.started": "2025-01-31T09:40:34.246998Z",
          "shell.execute_reply": "2025-01-31T09:40:59.558728Z"
        },
        "id": "US6Fk5vKRsF3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate classification report\n",
        "target_names = class_names  # Use the class names from the dataset\n",
        "report = classification_report(y_true, y_pred, target_names=target_names)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T09:40:59.561434Z",
          "iopub.execute_input": "2025-01-31T09:40:59.56168Z",
          "iopub.status.idle": "2025-01-31T09:40:59.575994Z",
          "shell.execute_reply.started": "2025-01-31T09:40:59.561656Z",
          "shell.execute_reply": "2025-01-31T09:40:59.575328Z"
        },
        "id": "auwEjjUoRsF3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T09:40:59.57717Z",
          "iopub.execute_input": "2025-01-31T09:40:59.577385Z",
          "iopub.status.idle": "2025-01-31T09:40:59.869747Z",
          "shell.execute_reply.started": "2025-01-31T09:40:59.577366Z",
          "shell.execute_reply": "2025-01-31T09:40:59.869007Z"
        },
        "id": "9fXyX0L5RsF3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot accuracy and loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy subplot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss subplot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T09:43:12.341655Z",
          "iopub.execute_input": "2025-01-31T09:43:12.341976Z",
          "iopub.status.idle": "2025-01-31T09:43:12.749927Z",
          "shell.execute_reply.started": "2025-01-31T09:43:12.341953Z",
          "shell.execute_reply": "2025-01-31T09:43:12.749105Z"
        },
        "id": "PcGCJUcJRsF3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/kaggle/working/final_model.h5\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T09:45:48.980854Z",
          "iopub.execute_input": "2025-01-31T09:45:48.981198Z",
          "iopub.status.idle": "2025-01-31T09:45:49.629026Z",
          "shell.execute_reply.started": "2025-01-31T09:45:48.981172Z",
          "shell.execute_reply": "2025-01-31T09:45:49.628135Z"
        },
        "id": "tR8tD__iRsF3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "dbW2hFvERsF3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "1vP3kse3RsF3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Load the dataset without batching (for splitting purposes)\n",
        "# full_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "#     total_folder,\n",
        "#     image_size=(224,224),\n",
        "#     batch_size=None,\n",
        "#     label_mode='categorical',\n",
        "#     shuffle=False,\n",
        "#     class_names=class_order\n",
        "# )\n",
        "\n",
        "# # Convert the dataset to lists of images and labels\n",
        "# images = []\n",
        "# labels = []\n",
        "# for img, label in full_ds:\n",
        "#     images.append(img.numpy())\n",
        "#     labels.append(label.numpy())\n",
        "\n",
        "# # Convert to NumPy arrays\n",
        "# images = np.array(images)\n",
        "# labels = np.array(labels)\n",
        "\n",
        "# # Split into training, validation, and test sets (e.g., 70% train, 15% val, 15% test)\n",
        "# train_images, temp_images, train_labels, temp_labels = train_test_split(\n",
        "#     images, labels, test_size=0.3, random_state=42, stratify=labels\n",
        "# )\n",
        "# val_images, test_images, val_labels, test_labels = train_test_split(\n",
        "#     temp_images, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        "# )\n",
        "\n",
        "# # Create TensorFlow datasets\n",
        "# train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(32)\n",
        "# val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).batch(32)\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(32)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T06:42:38.501824Z",
          "iopub.execute_input": "2025-01-31T06:42:38.502181Z",
          "execution_failed": "2025-01-31T06:46:01.877Z"
        },
        "id": "H9slaQa-RsF3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "OD3QtMjQRsF4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Evaluate the model\n",
        "# train_loss, train_accuracy = model.evaluate(train_ds, verbose=1)\n",
        "# print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# test_loss, test_accuracy = model.evaluate(val_orig_ds, verbose=1)\n",
        "# print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "# import numpy as np\n",
        "\n",
        "# # Predict on validation set\n",
        "# y_pred = model.predict(val_orig_ds)\n",
        "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T06:36:47.2041Z",
          "iopub.status.idle": "2025-01-31T06:36:47.204403Z",
          "shell.execute_reply": "2025-01-31T06:36:47.204274Z"
        },
        "id": "rA79HldqRsF4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Extract true labels (one-hot to class indices)\n",
        "# y_true = np.concatenate([y.numpy() for _, y in val_orig_ds], axis=0)\n",
        "# y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "# # Classification report\n",
        "# print(classification_report(y_true, y_pred_classes, target_names=class_order))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-31T06:36:47.205211Z",
          "iopub.status.idle": "2025-01-31T06:36:47.205516Z",
          "shell.execute_reply": "2025-01-31T06:36:47.205406Z"
        },
        "id": "hkguk_etRsF4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "gIPmidSjRsF4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "9M7dhMuGRsF4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Count samples per class in the balanced dataset\n",
        "# balanced_class_counts = defaultdict(int)\n",
        "# for _, labels in balanced_train_ds.unbatch():\n",
        "#     class_index = np.argmax(labels.numpy())\n",
        "#     balanced_class_counts[class_index] += 1\n",
        "\n",
        "# print(\"Balanced class counts:\", balanced_class_counts)\n",
        "len(balanced_train_ds)*32/7"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T16:55:52.085541Z",
          "iopub.execute_input": "2025-01-30T16:55:52.085833Z",
          "iopub.status.idle": "2025-01-30T16:55:52.091332Z",
          "shell.execute_reply.started": "2025-01-30T16:55:52.085801Z",
          "shell.execute_reply": "2025-01-30T16:55:52.090483Z"
        },
        "id": "FhJESzwRRsF4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T17:53:34.049474Z",
          "iopub.status.idle": "2025-01-30T17:53:34.049765Z",
          "shell.execute_reply": "2025-01-30T17:53:34.049651Z"
        },
        "id": "YOJdhpMNRsF5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# Define augmentation layers\n",
        "flip_layer = tf.keras.layers.RandomFlip(mode=\"horizontal_and_vertical\")\n",
        "rotate_layer = tf.keras.layers.RandomRotation(0.2)\n",
        "zoom_layer = tf.keras.layers.RandomZoom(0.2)\n",
        "\n",
        "# Augmentation function\n",
        "def augment(image):\n",
        "    image = flip_layer(image)\n",
        "    image = rotate_layer(image)\n",
        "    image = zoom_layer(image)\n",
        "    return image\n",
        "\n",
        "# Load the dataset without batching\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=None,  # Load as individual samples\n",
        "    label_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Get class counts\n",
        "class_counts = defaultdict(int)\n",
        "for _, labels in train_ds:  # No need for unbatch() since batch_size=None\n",
        "    class_index = np.argmax(labels.numpy())\n",
        "    class_counts[class_index] += 1\n",
        "\n",
        "# Find the maximum class count\n",
        "max_samples = max(class_counts.values())\n",
        "\n",
        "# Create a directory to save the augmented dataset\n",
        "total_aug_folder = \"/kaggle/working/total_augmented_dataset\"\n",
        "os.makedirs(total_aug_folder, exist_ok=True)\n",
        "\n",
        "# Create class subdirectories\n",
        "class_names = train_ds.class_names\n",
        "for class_name in class_names:\n",
        "    os.makedirs(os.path.join(total_aug_folder, class_name), exist_ok=True)\n",
        "\n",
        "# Function to save augmented images\n",
        "def save_augmented_images(dataset, class_index, target_count, class_name):\n",
        "    class_dir = os.path.join(total_aug_folder, class_name)\n",
        "    count = 0\n",
        "    for image, label in dataset:\n",
        "        if tf.argmax(label) == class_index:\n",
        "            augmented_image = augment(image)\n",
        "            tf.keras.utils.save_img(\n",
        "                os.path.join(class_dir, f\"aug_{count}.jpg\"),\n",
        "                augmented_image\n",
        "            )\n",
        "            count += 1\n",
        "            if count >= target_count:\n",
        "                break\n",
        "\n",
        "# Save augmented images for each class\n",
        "for class_index, class_name in enumerate(class_names):\n",
        "    print(f\"Augmenting and saving images for class: {class_name}\")\n",
        "    save_augmented_images(train_ds, class_index, max_samples, class_name)\n",
        "\n",
        "print(\"Augmented dataset saved to:\", total_aug_folder)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T17:10:35.541315Z",
          "iopub.execute_input": "2025-01-30T17:10:35.54167Z",
          "iopub.status.idle": "2025-01-30T17:29:39.743082Z",
          "shell.execute_reply.started": "2025-01-30T17:10:35.541645Z",
          "shell.execute_reply": "2025-01-30T17:29:39.742165Z"
        },
        "id": "wdclEpJ9RsF5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define image size and batch size\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "class_order = ['akiec', 'bcc', 'nv', 'df', 'mel', 'vasc', 'bkl']\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "    return image, label\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "total_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    total_folder,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',\n",
        "    shuffle=False,\n",
        "    class_names=class_order\n",
        ").map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T17:31:41.441885Z",
          "iopub.execute_input": "2025-01-30T17:31:41.442193Z",
          "iopub.status.idle": "2025-01-30T17:31:43.256135Z",
          "shell.execute_reply.started": "2025-01-30T17:31:41.442171Z",
          "shell.execute_reply": "2025-01-30T17:31:43.255475Z"
        },
        "id": "socmjsxGRsF5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, BatchNormalization, LeakyReLU, Dropout\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.applications import EfficientNetB0\n",
        "# from tensorflow.keras.optimizers import Adam, SGD\n",
        "# from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# import tensorflow as tf\n",
        "# # Load DenseNet121 base model (you can use DenseNet169, DenseNet201, etc.)\n",
        "# from tensorflow.keras.applications import DenseNet121\n",
        "\n",
        "# model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "# model.trainable = True  # Freeze all layers initially\n",
        "# total_ds = total_ds.apply(tf.data.experimental.ignore_errors())\n",
        "# val_ds = val_ds.apply(tf.data.experimental.ignore_errors())\n",
        "# x = model.output\n",
        "# x = GlobalAveragePooling2D()(x)\n",
        "# x = Dense(128, activation='relu', name=\"dense_128_relu\")(x)\n",
        "# x = Dropout(0.5, name=\"dropout_50\")(x)\n",
        "# predictions = Dense(7, activation='softmax', name=\"dense_soft_max\")(x)  # Replaced NUM_CLASSES with 7\n",
        "# model = tf.keras.Model(inputs=model.input, outputs=predictions)\n",
        "# optimizer = Adam(learning_rate=1e-3)\n",
        "# model.compile(\n",
        "#     optimizer=optimizer,\n",
        "#     loss='categorical_crossentropy',\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# # Callbacks\n",
        "# reduce_lr = ReduceLROnPlateau(\n",
        "#     monitor='val_accuracy',\n",
        "#     factor=0.5,\n",
        "#     patience=5,\n",
        "#     verbose=1,\n",
        "#     min_lr=1e-7\n",
        "# )\n",
        "\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_accuracy',\n",
        "#     patience=9,\n",
        "#     restore_best_weights=True,\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# # Train the model initially with base layers frozen\n",
        "# history = model.fit(\n",
        "#     total_ds,\n",
        "#     validation_data=val_ds,\n",
        "#     epochs=5,\n",
        "#     callbacks=[reduce_lr, early_stopping],\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# # Gradual unfreezing: Start unfreezing layers one-by-one or in batches\n",
        "# for layer in model.layers[-30:]:  # Unfreeze the last 30 layers\n",
        "#     layer.trainable = True\n",
        "\n",
        "# # Recompile with a lower learning rate for fine-tuning\n",
        "# fine_tune_optimizer = SGD(learning_rate=1e-4, momentum=0.9)\n",
        "# model.compile(\n",
        "#     optimizer=fine_tune_optimizer,\n",
        "#     loss='categorical_crossentropy',\n",
        "#     metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# # Fine-tune the model\n",
        "# early_stopping_fine = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_accuracy',\n",
        "#     patience=5,\n",
        "#     restore_best_weights=True,\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# reduce_lr_fine = ReduceLROnPlateau(\n",
        "#     monitor='val_accuracy',\n",
        "#     factor=0.5,\n",
        "#     patience=3,\n",
        "#     verbose=1,\n",
        "#     min_lr=1e-7\n",
        "# )\n",
        "\n",
        "\n",
        "# # Step 5: Compile and train\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history_fine = model.fit(\n",
        "#     total_ds,\n",
        "#     validation_data=val_ds,\n",
        "#     epochs=5,\n",
        "#     callbacks=[reduce_lr_fine, early_stopping_fine],\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "# # Evaluate the model\n",
        "# train_loss, train_accuracy = model.evaluate(balance_train_ds, verbose=1)\n",
        "# print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# test_loss, test_accuracy = model.evaluate(val_ds, verbose=1)\n",
        "# print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# from sklearn.metrics import classification_report\n",
        "# import numpy as np\n",
        "\n",
        "# # Predict on validation set\n",
        "# y_pred = model.predict(val_ds)\n",
        "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# # Extract true labels (one-hot to class indices)\n",
        "# y_true = np.concatenate([y.numpy() for _, y in val_original_ds], axis=0)\n",
        "# y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "# # Classification report\n",
        "# print(classification_report(y_true, y_pred_classes, target_names=class_order))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-30T18:02:36.059208Z",
          "iopub.execute_input": "2025-01-30T18:02:36.059618Z"
        },
        "id": "uUXOlu4YRsF5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "hGzrTHljRsF6"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}